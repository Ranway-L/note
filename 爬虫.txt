#Request库入门
		
		#Request库的七个主要方法
		requests.request()
		requests.get()
		requests.head()
		requests.post()
		requests.put()
		requests.patch()  局部修改请求	
		requests.delete()
		
		
		r = requests.get(url)
		构造一个向服务器请求资源的Request对象（Request库内部生成）
		返回的内容用变量r表示，是一个Response对象，包含从服务器返回的所有相关资源
		
		requests.get(url,params=None,**kwargs)
		
		#Response对象的属性
			r.status_code HTTP请求的返回状态，200为成功，非200为失败
			r.text HTTP响应内容的字符串形式，即，url对应的编码内容
			r.encoding 从HTTP header中猜测出来的编码方式
			r.apparent_encoding 从内容中分析出的响应内容编码方式（备选编码方式）（往往更加准确）
			r.content HTTP响应内容的二进制形式
			
		#Requests库的异常
			requests.ConnectionError 网络连接错误异常，如DNS查询失败、拒绝连接等
			requests.HTTPError HTTP错误异常
			requests.URLRequired URL缺失异常
			requests.TooManyRedirects 超过最大重定向次数，产生重定向异常
			requests.ConnectTimeout 连接远程服务器超时异常
			requests.Timeout 请求URL超时，产生超时异常
			
		r.raise_for_status() 如果不是200，产生异常requests.HTTPError
		
		#爬取网页的通用代码框架
		def getHTMLText(url):
			try:
				r = requests.get(url,timeout = 30)
				r.raise_for_status()
				r.encoding = r.apparent_encoding
				return r.text
			except:
				return"产生异常"
				
				
		#HTTP协议
			URL格式 http://host[:port][path]
			host:合法的Internet主机域名或IP地址
			port：端口号，缺省端口为80
			path：请求资源的路径
		
#网络爬虫规则：
		Robots协议
		/robots.txt
		#注释，*代表所有，/代表根目录
		User-agent:*
		Disallow:/
		
	#伪装浏览器进行爬虫访问
		修改头部字段
		kv = {'user-agent':'Mozilla/5.0'}
		r = requests.get(url,headers=kv)
		
	#百度搜索全代码
		import requests
		keyword = 'python'
		try:
			kv = {'wd':keyword}
			r = r.requests.get("http://www.baidu.com/s" ,params=kv)
			print(r.requests.url)
			r.raise_for_status()
			print(len(r.text))
		except:
			print("爬取失败")
			
			//360搜索将键值由wd改为q

	#图片爬取全代码
		import os
		import requests
		url = "http://afafafa.com/1321311313.jpg"
		root = "D://pics//"
		path = root + url.split('/')[-1]
		try:
			if not os.path.exists(root):
				os.mkdir(root)
			if not os.path.exists(path):
				r = requests.get(url)
				with open(path,'wb') as f:
					f.write(r.conten)
					f.close()
					print("文件保存成功")
		except:
			print("爬取失败")
#Beautiful Soup库




#信息组织与提取方法
	XML 通过标签形式构建信息，可注释
	JSON 有类型的键值对 key: value，键值对可以嵌套，不可注释
	YAML 无类型键值对，可注释
	
	
	#中国大学排名爬取代码
			#CrawUnivRankingB.py
			import requests
			from bs4 import BeautifulSoup
			import bs4
			 
			def getHTMLText(url):
				try:
					r = requests.get(url, timeout=30)
					r.raise_for_status()
					r.encoding = r.apparent_encoding
					return r.text
				except:
					return ""
			 
			def fillUnivList(ulist, html):
				soup = BeautifulSoup(html, "html.parser")
				for tr in soup.find('tbody').children:
					if isinstance(tr, bs4.element.Tag):
						tds = tr('td')
						ulist.append([tds[0].string, tds[1].string, tds[3].string])
			 
			def printUnivList(ulist, num):
				tplt = "{0:^10}\t{1:{3}^10}\t{2:^10}"
				print(tplt.format("排名","学校名称","总分",chr(12288)))
				for i in range(num):
					u=ulist[i]
					print(tplt.format(u[0],u[1],u[2],chr(12288)))
				 
			def main():
				uinfo = []
				url = 'https://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html'
				html = getHTMLText(url)
				fillUnivList(uinfo, html)
				printUnivList(uinfo, 20) # 20 univs
			main()
	
	
	
	
	
	